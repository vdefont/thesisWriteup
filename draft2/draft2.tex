\documentclass[12pt]{pom_thesis}

\newcommand{\sdiv}{\mathrm{div}}
\newcommand{\scurl}{\mathrm{curl}}
\newcommand{\sker}{\mathrm{ker}}
\newcommand{\ssim}{\mathrm{im}}
\newcommand{\sdiag}{\mathrm{diag}}

% TITLE PAGE

\author{Victor de Fontnouvelle}
\advisor{Vin De Silva}
\title{Victor's Senior Thesis}

\begin{document}

\maketitle


% ABSTRACT

\begin{abstract}

We will explore various methods of analyzing high-dimensional data. We'll investigate techniques that make inferences about the underlying structure of the data, cluster the data, or reduce the dimension of the data. We'll apply these methods to real-world datasets, compare the methods, and suggest improvements to the methods.

\end{abstract}


% TABLE OF CONTENTS

\pagenumbering{roman}
\tableofcontents

\newpage
\pagenumbering{arabic}


% INTRO CHAPTER

\begin{chapter}{Introduction}
\label{chapter:intro}

\section{Inference About the Structure of the Data}

Cohomology analysis and Helmholtz decomposition provide insight on the structure of the data. Cohomology analysis provides a broader framework for detecting clusters, holes, and higher-order features within the data. Helmholtz decomposition is a specific application of cohomology analysis which seeks to find an ordered list which best accounts for a weighted graph.

\section{Clustering}

Mapper clusters the data, by mapping clusters of points onto intervals on the real line using a filter function, and connecting overlapping clusters.

\section{Dimensionality Reduction}

Calculating the eigenvalues of the laplacian matrix reduces the dimension of the data. The laplacian is a symmetric matrix encoding the pairwise distances between points, normalized by row. This matrix can be thought of as a linear operator encoding heat flows. Given an input of initial temperatures, it outputs the changes in temperatures after one time step. Eigenvectors corresponding to low eigenvalues thus correspond to stable temperature configurations. The eigenvectors are perpendicular, and thus eigenvectors corresponding to slightly higher eigenvalues often capture geometric structure existing in the data. Additionally, nearby points will have similar values in the eigenvectors, and thus the eigenvectors are also a useful tool for reducing the dimension of the data.

\end{chapter}


% LIT REVIEW CHAPTER

\begin{chapter}{Review of Literature}
\label{chapter:litReview}

Papers by Gunnar Carlsson \cite{carlssonMain} and Vin de Silva \cite{deSilvaCohomology} explain the intuition behind cohomolgy analysis, and provides several examples. Carlsson \cite{carlssonCompute} and deSilva \cite{deSilvaCohomology} also describe the algorithm used to compute cohomology, which will be useful if I choose to implement it.

Curto \cite{curto} and Ulmer \cite{biologicalModels} both provide various examples of the uses of homology analysis, both in detecting underlying structure, and in providing fingerprints that identify different phenomena. Curto analyzes a dataset representing connection strengths between neurons in rats responsible for spatial recognition. Curto first keeps a certain proportion of edges $\rho$ s.t. $0<\rho<1$, keeping those edges which are the strongest. Curto then runs cohomology analysis on this graph to determine the Betti curve. Curto determined that the Betti curve obtained from spatially organized neurons is different than the Betti curve that would be obtained from neurons with random structure. Cohomolgy analysis thus provides a method for detecting spatial neurons. Ulmer uses cohomology analysis to evaluate two different models of social interaction for aphids roaming in a dish. Standard measures that compare the models to real data include angular momentum, and average distance to closest neighbor. Ulmer found that cohomology analysis provided an equally strong measurement for assessing model accuracy.

A paper by Jiang \cite{hodge} explains the Helmholtz decomposition that converts ranked data into ordinal data. It provides both the algorithm for the decomposition, as well as three examples of its use. Jiang uses Helmholtz decomposition to rank movies. Most users rate several movies, so each time a user rated two movies, this introduced an edge from one movie to the other indicating the user's preference. Jiang used Helmholtz decomposition to create an absolute index of currency values based off trading rates. Jiang also used Helmholtz decomposition to rank websites based off of connection strengths. Jiang found that Helmholtz decomposition performed as well as some of the standard methods for website ranking, indicating that it is a useful tool.

Carlsson \cite{carlssonMain} also explains the Mapper tool, and provides examples of its use. I have already used the algorithm described in this paper to analyze several datasets.

Singh \cite{laplacian} describes the intuition behind the laplacian analysis, which I have also implemented.

\end{chapter}


% HELMHOLTZ 

\begin{chapter}{Helmholtz Decomposition}
\label{chapter:helmholtz}

\section{Explanation of Helmholtz Decomposition}

\subsection{Intuition}

The Hodge Decomposition Theorem provides that $\bar{Y}$ can be expressed as the sum of three orthogonal components:

\begin{enumerate}
	\item The gradient flow $G$
	\item The curl flow $C$
	\item The harmonic flow $H$
\end{enumerate}

The gradient flow $G$ represents a comparison matrix that corresponds directly to a vector $v$ where each item is assigned a real value.
$G$ is the gradient of $v$, thus each entry is computed as $G[ij] = v[j] - v[i]$.

The harmonic flow $H$ represents a ranking that is curl-free and divergence-free.
This means that any three items in $H$ will have pairwise rankings that are logically consistent.
Specifically, $H[ij] + H[jk] + H[ki] = 0, \forall i,j,k$.
Because $\sdiv(H)=0$, $H$ contains plausible values in the sense that it could have been produced by real-world data.
A harmonic flow thus indicates that there are cycles in the graph with more than three edges, where the edge weights don't sum to zero.

The curl flow $C$ is the image of $\scurl^*$, the adjoint of the curl.
Nonzero values of C thus indicate cycles of length three whose edge weights don't sum to zero.

The gradient flow provides the ranking that minimizes the least-squared residual, while the harmonic and curl flows characterize the residual.
Specifically, $\bar{Y} - H = G + C$.
A large curl flow indicates that the ordering of items ranked closely together is unreliable,
while a large harmonic flow indicates that the ordering of items ranked further apart is unreliable.
For example, if the curl flow is large but the harmonic flow is small, this indicates that the ranking is valid at a larger scare,
but that the specific ordering of closely-ranked alternatives isn't very precise.

\subsection{Representation of Matrices}

It is necessary to have matrix representations of $\delta_0$ (grad), $\delta_1$ (curl), $\delta_0^*$, and $delta_1^*$ in order to compute the gradient, curl, and harmonic flows.
Throughout this section, $f$, $\alpha$, and $A$ refer to maps from edges, vertices, and triples into $\mathbb{R}$, respectively.

\bigskip

The gradient operator assigns values to edges based on the difference of the values of the endpoints.
Specifically, $\alpha((a,b)) = - f(a) + f(b)$.
For example, when the graph has four vertices and all edge weights are nonzero, the matrix representation of the gradient is

\[
  \begin{bmatrix}
    -1 & 1 & 0 & 0 \\
    -1 & 0 & 1 & 0 \\
    -1 & 0 & 0 & 1 \\
    0 & -1 & 1 & 0 \\
    0 & -1 & 0 & 1 \\
    0 & 0 & -1 & 1
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b \\ c \\ d
  \end{bmatrix}
  =
  \begin{bmatrix}
    ab \\ ac \\ ac \\ bc \\ bd \\ cd
  \end{bmatrix}
\]

We can verify the correctness of this matrix by examination.
For example, the first row corresponds to the edge $(a, b)$ and thus we want $\alpha((a,b)) = - f(a) + f(b)$.
Indeed, the value $-1$ in the first column corresponds to $a$, and the value $+1$ in the second column corresponds to $b$.
If any of the edge weights are zero, we remove the corresponding row from the gradient matrix.
For example, if $W((a,b)) = 0$, we would remove the first row from the matrix above.

\bigskip

The curl operator assigns values to triples based off the values of edges.
Specifically, $A((a,b,c)) = \alpha(b,c) - \alpha(a,c) + \alpha(a,b)$.
For example, when the graph has four vertices and all edge weights are nonzero, the matrix representation of the curl is:

\[
  \begin{bmatrix}
    1 & -1 & 0 & 1 & 0 & 0 \\
    1 & 0 & -1 & 0 & 1 & 0 \\
    0 & 1 & -1 & 0 & 0 & 1 \\
    0 & 0 & 0 & 1 & -1 & 1
  \end{bmatrix}
  \begin{bmatrix}
    ab \\ ac \\ ad \\ bc \\ bd \\ cd
  \end{bmatrix}
  =
  \begin{bmatrix}
    abc \\ abd \\ acd \\ bcd
  \end{bmatrix}
\]

To check correctness, we examine the triple $(a,b,c)$.
We would want to assign this triple the value $\alpha((b,c)) - \alpha((a,c)) + \alpha((b,c))$.
We can verify that indeed the row corresponding to edge $(a,b)$ has values $+1$, $-1$, and $+1$ at the columns corresponding to the edges $(b,c)$, $(a,c)$, and $(b,c)$.
If any of the edge weights are zero, we remove the corresponding column from the matrix.
If any of the triples contain an edge with zero weight, we remove the corresponding row from the matrix.

\bigskip

We now compute $\delta_0^*$ and $\delta_1^*$.
Let $M_{\delta_0^*}$ and $M_{\delta_0}$ denote the matrix representations of $\delta_0^*$ and $\delta_0$, respectively.
By definition of the adjoint operator, we must have that:
\begin{equation}\label{equation:gradAdj}
<\delta_0f,\alpha> = <f,\delta_0^*\alpha>
\end{equation}
$\delta_0f$ and $\alpha$ are in $C^1$, thus their inner product includes multiplication by edge weights, and is computed as $\sum_{i,j}(\delta_0f)_{ij}\alpha_{ij}W_{ij}$.
$f$ and $\delta_0^*$ are in $C^0$, thus their inner product does not include edge weights.
So in order for \ref{equation:gradAdj} to be valid, we need $\delta_0^*$ to include multiplication by edge weights.
Then multiplying the rows of $M_{\delta_0}$ by the corresponding edge weights and transposing the result will yield $M_{\delta_0^*}$.
Specifically, $M_{\delta_0^*} = (D M_{\delta_0})^T$ where $D = \sdiag(W_{ij})$ for all nonzero edges $(i,j)$.

\bigskip

Similarly, in the case of the curl operator we must have:
\begin{equation}\label{equation:curlAdj}
<\delta_1\alpha,A> = <\alpha,\delta_1^*A>
\end{equation}
This time $\alpha$ and $\delta_1^*A$ are in $C^1$ while $\delta_1\alpha$ and $A$ are in $C^2$.
Thus only the inner product $<\alpha, \delta_1^*A>$ includes multiplication by edge weights.
Then dividing the rows of $M_{\delta_1}$ by the corresponding edge weights and transposing the result will yield $M_{\delta_1^*}$.
Specifically, $M_{\delta_1^*} = (M_{\delta_1} D')^T$ where $D' = \sdiag(\frac{1}{W_{ij}})$ for all nonzero edges $(i,j)$.

\subsection{Computation of the Gradient, Curl, and Harmonic Flows}

Having defined the necessary matrix representations, we are ready to compute the three flows, which we denots $G$, $C$, and $H$.
The gradient flow is the orthogonal projection of $Y$ onto $\ssim(\delta_0)$.
So $G = \delta_0(\delta_0^*\delta_0)^+\delta_0^*Y$, where $^+$ denotes the Moore-Penrose pseudo-inverse.

\bigskip

The curl flow is the orthogonal projection on to $\ssim(\delta_1^*)$.
We thus hope to find $A \in C^2$ which is the least squares solution to $Y = \delta_1^*A$.
Note that this equation is in the inner product space $C_1$ which includes multiplication by edge weights.
To solve this equation in the standard inner product space, it is equivalent to multiply both sides by $\sqrt{D}$, and solve $\sqrt{D}Y = \sqrt{D}\delta_1^*A$, where $D = \sdiag(W_{ij})$.
Then $C = \delta_1^*A$.

\bigskip

The harmonic flow is $\sker \delta_0^* \cap \sker \delta_1 = \sker \Delta_1$ where $\Delta_1 = \delta_1^*\delta_1 + \delta_0\delta_0^*$.
Then $D\Delta_1 = \Delta_1^T D$.
So $D^{1/2}\Delta_1D^{-1/2} = D^{-1/2}\Delta_1^TD^{1/2}$.
So $S = D^{1/2}\Delta_1D^{-1/2}$ is symmetric, and we can compute its pseudo-inverse.
We thus have that the matrix representation of the projection in to $\sker \Delta_1$ is $P = D^{1/2}(S^+S)D^{1/2}$.
So $H = (I - P)Y$.

\section{Application of Helmholtz Decomposition}

\end{chapter}


% COHOMOLOGY 

\begin{chapter}{Cohomology Analysis}
\label{chapter:cohomology}

\section{Explanation of Cohomology Analysis}

\section{Application of Cohomology Analysis}

\end{chapter}


% MAPPER 

\begin{chapter}{Mapper}
\label{chapter:mapper}

\section{Explanation of Mapper}

\section{Explanation of Mapper}

\end{chapter}


% LAPLACIAN EIGEN ANALYSIS

\begin{chapter}{Laplacian Eigenvector Analysis}
\label{chapter:methods}

\section{Explanation of Laplacian Eigenvector Analysis}

\section{Application of Laplacian Eigenvector Analysis}

\end{chapter}


% DISCUSSION

\begin{chapter}{Discussion}
\label{chapter:discussion}

\end{chapter}


% CONCLUSION

\begin{chapter}{Conclusion}
\label{chapter:conclusion}

\end{chapter}


% BIBLIOGRAPHY

\begin{thebibliography}{9}


\bibitem{carlssonMain}
Carlsson, Gunnar.
"Topology and data."
\textit{Bulletin of the American Mathematical Society}
46.2 (2009): 255-308.

\bibitem{carlssonCompute}
Zomorodian, Afra, and Gunnar Carlsson.
"Computing persistent homology."
\textit{Discrete \& Computational Geometry}
33.2 (2005): 249-274.

\bibitem{deSilvaCohomology}
De Silva, Vin, Dmitriy Morozov, and Mikael Vejdemo-Johansson.
"Persistent cohomology and circular coordinates."
\textit{Discrete \& Computational Geometry}
45.4 (2011): 737-759.


\bibitem{curto}
Giusti, Chad, et al.
"Clique topology reveals intrinsic geometric structure in neural correlations."
\textit{Proceedings of the National Academy of Sciences}
112.44 (2015): 13455-13460.

\bibitem{biologicalModels}
Ulmer, M., Lori Ziegelmeier, and Chad M. Topaz.
"Assessing biological models using topological data analysis."
\textit{arXiv preprint arXiv:1811.04827}
(2018).


\bibitem{hodge} 
Jiang, Xiaoye, et al. "Statistical ranking and combinatorial Hodge theory."
\textit{Mathematical Programming}
127.1 (2011): 203-244.


\bibitem{laplacian}
Singh, Gurjeet, Facundo Mémoli, and Gunnar E. Carlsson.
"Topological methods for the analysis of high dimensional data sets and 3d object recognition."
\textit{SPBG.}
2007.

\end{thebibliography}


\end{document} 